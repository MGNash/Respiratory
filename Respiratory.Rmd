---
title: "Clinical Trial of Patients with Respiratory Illness"
author: "Michael Nash"
date: "September 13, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(gpairs)
library(ggplot2)
library(nlme)
library(mfp)
library(geepack)
library(aod)
library(car)
library(ResourceSelection)
library(printr)
library(broom)
change.coef <-function(mod1,mod2){return(round(100*(coef(mod2)-coef(mod1))/coef(mod1),digits=2))}
gpairs(data.frame(1:2,1:2))
```


I present the following as an example of how to analyze categorical data with repeated measures in R. I use a didactic dataset originally found in chapter 15 of Stokes, Davis and Koch (1995) and also included in Fitzmaurice, Laird & Ware (2011). It can be found on the website for the latter text (https://content.sph.harvard.edu/fitzmaur/ala/). Here is an excerpt from this website explaining how the data were obtained:

*The data are from a clinical trial of patients with respiratory illness, where 111 patients from two different clinics were randomized to receive either placebo or an active treatment. Patients were examined at baseline and at four visits during treatment. At each examination, respiratory status (categorized as 1 = good, 0 = poor) was determined.*

The treatment outcome is the patient's respiratory status. I sometimes refer to good and poor respiratory status as GRS and PRS, respectively. The purpose of the trial is to assess the effect of the active treatment on respiratory status, although one might also be interested in how other factors are related respiratory status. Patients' gender, respiratory status and age at a pre-treatment baseline visit, and at which of two study centers they were enrolled are also known.

I will begin by reading the data into R from the text file supplied on the textbook website. I will perform some exploratory data analysis, followed by statistical analysis to determine the effect of the treatment and subject characteristics on the outcome of respiratory status.

```{r}
setwd("C:/Users/Michael Nash/Documents/job search/2018/Github_proj/Respiratory")
source("HighstatLib.r")
data1 = read.table("respir.txt",skip = 30)
colnames(data1) = c("Center","ID_C","Treatment","Gender","Age","Base","V1","V2","V3","V4")
data1$Center = factor(data1$Center)
data1$Treatment = factor(data1$Treatment, levels = c("P","A"))
data1$Gender = factor(data1$Gender)
data1$Base = factor(data1$Base,levels = c(0,1),labels=c("Poor","Good"))
data1$V1 = factor(data1$V1,levels = c(0,1),labels=c("Poor","Good"))
data1$V2 = factor(data1$V2,levels = c(0,1),labels=c("Poor","Good"))
data1$V3 = factor(data1$V3,levels = c(0,1),labels=c("Poor","Good"))
data1$V4 = factor(data1$V4,levels = c(0,1),labels=c("Poor","Good"))
data1$ID = data1$ID_C + 56*as.numeric(data1$Center == 2)
data1 = data1[,-2]
data1 = data1[c(10,1:9)]
head(data1)
```

The first few rows of data in a 'wide' format, with outcome measurements taken at the five timepoints for each subject shown in a single row of the table. Although only the first six rows are shown, the table contains 111 rows, one for each subject. From left to right, the dataset contains the following variables:  

Center - Each subject is enrolled at one of two study centers, numbered 1 and 2.  
ID - Each subject has a unique ID, numbering from 1 to 111.  
Treatment - Each subject receives one of two treatments: A for active or P for placebo.  
Age - Subject's age in years at baseline visit.  
Base - Subject's respiratory status at baseline visit.  
V1, V2, V3, V4 - Subject's respiratory status at one of four visits.  

Confusingly, subject IDs number 1 to 56 at the first study center and 1 to 55 at the other, such that pairs of subjects share an ID. I created a unique ID for each subject and removed the original, non-unique IDs.

```{r}
("Summary of \'Complete Cases\'")
summary(complete.cases(data1))
```

111 of 111 'complete cases' indicates that every subject has complete data.


```{r}
summary(data1[-c(1,5)])
```

The numbers of subjects at each of the two centers, and assigned to placebo and active treatments, are roughly equal. All subjects are male or female. Male subjects outnumber female subjects almost 4 to 1. Subjects with poor respiratory status outnumber those with good respiratory status at baseline, whereas the reverse is true at all subsequent visits. This could indicate that the treatment works, or just that the subjects tend to get better over time.

```{r}
hist(data1$Age, main = "Subject Age at Baseline", xlab = "Age (Years)")
```
```{r}
print("Summary statistics for baseline age:")
summary(data1$Age)
```

Subjects' ages range from 11 to 68 at baseline. I do not know how far apart post-treatment visits are from one another or from the baseline visit, so it is possible that subjects are actually older than this at later visits.

These data come from a randomized trial. The purpose of randomization in a clinical trial or any other experiment is to prevent confounding, which is when a third variable is related to both the outcome one wishes to understand and the exposure whose affect one is studying, distorting the effect of the exposure. For example, in a non-randomized trial, doctors might assign patients to therapies based on the seriousness of their conditions, with more severely ill patients receiving a more aggressive treatment. One would then say that the treatment condition is confounded with the severity of illness. One might observe that patients receiving the more aggressive treatment tend to have a worse prognosis and erroneously assume that the difference in outcomes is caused by the difference in treatment received, when it is really caused by the difference in disease severity.

Randomization avoids this situation by making individuals with different characteristics equally likely to be assigned to a given exposure condition, so that there are no other systematic differences between subjects exposed to different conditions as part of the experiment. Crucially, this is true for all possible characteristics of the subjects, even those that are unknown. For this reason, unlike an observational study, one can infer a causal relationship between exposure and outcome from the results of a randomized experiment.  The characteristics of subjects assigned to different treatments can still differ by chance, and it is worth checking whether this is the case for characteristics one can measure and suspect might be important in determining the outcome.

Below is a plot showing the pairwise relationships among the variables of treatment, study center, gender, baseline respiratory status ('base'), respiratory status at each post-treatment visit ('status'), and age. For each combination of categorical variables, a rectangle represents the overlap between two categories, with the size of the rectangle being proportional to the number of subjects contained in this overlap. A bar plot shows the number of individuals at each age grouped by each dichotomous variable. I will not be performing any hypothesis tests, because I already know that any differences between treatment groups occurred by chance, and they could still affect the results even if they are not statistically significant.

```{r}
# converting data to 'long' format
row = 0
datalong = data.frame(matrix(ncol=8,nrow=dim(data1)[1]*4))
colnames(datalong) = c(colnames(data1)[1:6],"Visit","Status")
for(i in 1:length(data1$ID)){
  for(j in 1:4){
    row = row + 1
    datalong[row,c(1:6)] = data1[i,1:6]
    datalong$Visit[row] = j
    datalong$Status[row] = data1[i,6+j]
  }
}
datalong$Center = factor(datalong$Center)
datalong$Treatment = factor(datalong$Treatment, levels = c(1,2), labels = c("P","A"))
datalong$Gender = factor(datalong$Gender, levels = c(1,2), labels = c("F","M"))
datalong$Base = factor(datalong$Base,levels = c(1,2),labels=c("Poor","Good"))
datalong$Status = factor(datalong$Status,levels = c(1,2),labels=c("Poor","Good"))
```
```{r}
plot1 = gpairs(datalong[,c(3,2,4,6,8,5)],upper.pars = list(conditional='barcode'),lower.pars = list(conditional='barcode'))
```

Groups assigned to active and placebo treatments have approximately equal proportions of good and poor respiratory status at baseline and of subjects at the two study centers. The age distribution seems to be similar for active treatment and placebo groups, and for every other variable represented. There seem to be more female subjects assigned to placebo than active treatment, which makes gender a possible confounder.

At the intersection of 'baseline' and 'status', the rectangles in the top left and bottom right represent visits at which an individual's current status is the same as their baseline status, whereas the top right and bottom left rectangles represent visits at which an individual's current status is different from their baseline status. The first two rectangles are larger, indicating that individuals with good respiratory status at baseline are more likely to have good respiratory status at a follow-up visit than those with poor respiratory status at baseline, and vice versa, as one would expect.

Female subjects are somewhat more likely to have poor respiratory status than male subjects, both at baseline and at follow-up. Subjects at center 1 are more likely to have poor respiratory status at baseline and at follow-up than those at center 2. Individuals receiving the active treatment are more likely to have good respiratory status at follow-up visits, but not at baseline, than those in the placebo group. This suggests the treatment may be working, although I will return to this question.

It is also not known how respiratory status changes over time in each treatment group. I have shown that subjects with poor respiratory status outnumber those with good respiratory status at baseline, and that the reverse is true at post-treatment visits overall, but so far this is all that is known about the pattern of respiratory status over time. The plot below shows the mean respiratory status at baseline (denoted as visit zero) and at each of the four post-treatment visits among subjects assigned to active and placebo treatments (visits 1 through 4).

```{r}
# Here's a long table of mean response
longmean = matrix(rep(NA,2*4*3),nrow=2*4)
colnames(longmean)=c("Visit","Treatment","Proportion")
i=0
for(treat in unique(datalong$Treatment)){
  for(visit in unique(datalong$Visit)){
    i = i+1
    longmean[i,]=c(visit,treat,mean(as.numeric(datalong$Status[which((datalong$Treatment == treat)&(datalong$Visit == visit))] == "Good")))
  }
}
longmean = with(datalong,rbind(c(0,"P",mean(as.numeric(Base[Treatment == "P"]=="Good"))),
                               c(0,"A",mean(as.numeric(Base[Treatment == "A"]=="Good"))),
                               longmean))
longmean[,1] = as.numeric(longmean[,1])
longmean[,3] = as.numeric(longmean[,3])
longmean = longmean[order(longmean[,2]),]
longmean.df = data.frame(Visit = as.numeric(longmean[,1]),Treatment = longmean[,2],Percent=100*as.numeric(longmean[,3]))
#longmean.df$Proportion = as.numeric(longmean[,"Proportion"])
#longmean.df$Visit = as.numeric(longmean.df$Visit)


# Here's a profile plot of mean response
ggplot(longmean.df, aes(as.numeric(Visit),as.numeric(Percent),group=Treatment, color=Treatment))+geom_line()+scale_y_continuous(name = "% Good Respitory Status")+scale_x_continuous(name = "Visit")

#(longmean.df)
```


The percentage of subjects with good respiratory status starts around 45 for both treatment groups. For the active treatment group, the share of patients with good respiratory status goes above 60% starting with the first post-treatment visit and stays there. For the placebo group, the percentage with good respiratory status fluctuates in a range between about 38% and 48% but never goes any higher. This suggests that individuals who receive the active treatment are more likely to have good respiratory status than those who do not.

However, this does not tell us whether differences in post-treatment respiratory status between treatment groups could have occurred due to chance or be explained by other factors. In order to answer these questions, I will fit a multivariable model of respiratory status. Because the outcome is binary, I will choose a logistic regression model, one of several models that can be used to model the probability of a binary outcome as the function of several predictor variables. A probit model would also be a reasonable choice.

The basic logistic regression model assumes that observations are independent, meaning that the outcome for each observation is unrelated to every other observation. The data set contains multiple observations for each individual. One would expect that two observations from the same individual would be more likely to have the same outcome than two observations from different individuals, violating the assumption of independence. Therefore, I will choose a model that accounts for different observations from the same individual being correlated with one another, which is known as autocorrelation.

There are two major types of models that can be used to model a binary outcome while accounting for autocorrelation. A marginal model predicts the mean response at any given combination of predictor levels, whereas a mixed model predicts the individual response. The coefficient estimates in a marginal model have a between-subject interpretation. This type of model can tell us, for example, the difference between the expected odds of good respiratory status between individuals assigned to different treatments with all other things being equal (i.e. the difference in the averages). The coefficient estimates in a mixed model have a within-subject interpretation. This type of model can tell us, for example, the expected difference in the odds of good respiratory status for the same individual if they are assigned to different treatments (i.e. the average of the differences).

An unfortunate feature of the mixed model is that the form of the dependence between different observations in the same subject over time must be specified. This is a problem for us, because I do not even know how far apart the visits occur in time, or whether the time intervals between them are the same. With a marginal model, one can use an unstructured correlation matrix, separately estimating the correlation among each pair of timepoints without making any assumptions about how this changes over time.

I will fit a marginal model with an unstructured correlation using the GEE (generalized estimating equations) method. The outcome will be the log odds of good respiratory status at a given post-treatment visit. This will allow us to measure the association of each covariate with the odds of good respiratory status when all others are held constant.

My procedure will be as follows:
0) Check for multicollinearity
1) Select the appropriate scale in which to include age based on the performance of polynomial terms in a main effects model. 
2) Evaluate interaction effects in a model containing all main effects and all interactions with treatment.
3) Evaluate main effects in a model containing interactions selected in the previous step.
4) Fit a final model containing only main effects and interactions selected in previous steps.
5) Evaluate the fit of the final model.
6) Interpret the final model.

Multicollinearity occurs when covariates are strongly correlated with one another, such that the value of one covariate can be strongly predicted from other covariates. When this occurs, coefficient estimates become numerically unstable and their standard errors become much larger. A generalized variance inflation factor (GVIF) is used to quantify the extent to which each covariate is correlated with others, with GVIF of 10 traditionally used as a cutoff to identify covariates which are so strongly correlated with others as to seriously interfere with accurately estimating model coefficients.

```{r}
datalong$Visit.f = factor(datalong$Visit)
datalong$Status.n = as.numeric(datalong$Status == "Good")
datalong$Age2 = with(datalong,(Age-mean(Age))^2)
datalong$Age3 = with(datalong,(Age-mean(Age))^3)
datalong$Age1 = with(datalong,(Age-mean(Age)))

mod.0.gee = geeglm(Status.n ~ Treatment + Visit.f + Center + Gender + Age + Base, id = ID, data = datalong, 
                   family ="binomial", corstr = "unstructured")

vif1 = myvif(mod.0.gee)
print("GVIF:")
vif1[,1]
```

Fortunately, multicolinearlity does not seem to be a problem for this set of variables. The function used to generate GVIFs comes from the public Github repository 'RCode_Master_UPO'of user PedroJ.

Next, I need to select the appropriate scale in which to include subject age. I don't know the form of the relationship between subject age and the odds of good respiratory status. It could be linear, it could be something else, or there might not be any relationship. I will evaluate polynomial terms of age in a main-effects only model (also containing treatment, visit number, center, gender, baseline status) up to the third order (i.e. linear, quadratic and cubic terms), because a third order polynomial can approximate a wide range of nonlinear relationships well. If the cubic term is significant (i.e. p < .05 for a test of the hypothesis that including this coefficient significantly adds to model fit), I will include all three terms. If the quadratic but not the cubic terms are significant, I will include only quadratic and linear terms. If quadratic and cubic terms are not significant, I will include only the linear term.

The ANOVA table below shows tests of the hypothesis that various polynomial terms for age improve model fit. Age1, 2 and 3 refer to linear, quadratic and cubic terms for age, respectively. 

```{r}
mod.1.gee = geeglm(Status.n ~ Treatment + Visit.f + Center + Gender + Age1 + Age2 + Age3 + Base, id = ID, data = datalong, 
                   family ="binomial", corstr = "unstructured")
anova(mod.1.gee)[5:7,]
```

The quadratic (chisq(1) = 9.77, p = .0018) but not the cubic term (chisq(1) = 0.32, p = 0.5739) significantly add to model fit. Therefore, I will model the quadratic relationship between age and odds of good respiratory status in the full model containing main effects and interactions.

Besides the possibility of confounding, another reason to consider other variables when understanding the relationship between the treatment condition and outcome is that this relationship may differ according to these other variables. When the effect of the treatment is different for subjects with different values of some other variable, one would say that interaction or effect modification is present. For example, if the treatment has a stronger effect on post-treatment respiratory status for individuals with PRS at baseline than those with GRS at baseline, one would say that there is a baseline status by treatment interaction, or that baseline status is an effect modifier.

I will now consider the interactions of treatment with visit, center, gender, age and baseline status in a model containing the main effects listed above with linear and quadratic terms for age. The ANOVA table below shows tests of the hypothesis that various interactions improve model fit. The age by treatment interactions are evaluated in a separate test because I wish to test the two terms (linear and quadratic) together.

```{r}
mod.2.gee =  geeglm(Status.n ~ Treatment + Visit.f + Center + Gender + Age1 + Age2 + Base + Treatment:Visit.f + Treatment:Center + Treatment:Gender + Treatment:Age1 + Treatment:Age2+ Treatment:Base, id = ID, data = datalong, 
                   family ="binomial", corstr = "unstructured")
anova(mod.2.gee)[c(8:10,13),]

print("Hypothesis: no age by treatment interaction:")
wald.test(vcov(mod.2.gee),coef(mod.2.gee),c(16,17))

#mod.2.gee.alt =  geeglm(Status.n ~ Treatment + Visit.f + Center + Gender + Age1 + Age2 + Base + Treatment:Visit.f + Treatment:Center + Treatment:Gender + Treatment:Base, id = ID, data = datalong, family ="binomial", corstr = "unstructured")
#anova(mod.2.gee,mod.2.gee.alt)

```

The visit by treatment interaction does not significantly improve model fit (chisq(3) = 3.15, p = 0.3688). I conclude that the effect of the treatment is the same at each visit. In other words, the pattern of change in respiratory status over time (over the post-treatment visits) does not differ for the two treatments. The treatment by center interaction does not significantly improve model fit (chisq(1) = 2.20, p = 0.1378). I conclude that the treatment effect does not differ by center, meaning that the treatment is equally effective when administered at either center. The treatment by age interactions for the linear and quadratic terms, considered together, do not significantly improve model fit (chisq(2) = 2.20, p = 0.1378). I conclude that the treatment effect does not depend on age, meaning that the treatment is equally effective at all ages. The treatment by baseline status interaction does not significantly improve model fit (chisq(1) = 1.45, p = 0.2279). I conclude that the treatment effect does not differ by baseline status, meaning that the treatment is equally effective for individuals with good and poor respiratory status before initiating treatment. The treatment by gender interaction significantly improves model fit (chisq(4.02), p = .0451). I conclude that the treatment is not equally effective for male and female patients. Based on these results, I will include the treatment by gender interaction and no others.

I will now consider the main effects of visit, center, age and baseline status in a model which also contains the main effects of treatment and gender and their interaction. The main effects of treatment and gender should be retained in the model regardless of whether they significantly add to model fit and are not interpretable because there exists an interaction between treatment and gender. The ANOVA table below shows tests of the hypothesis that various main effects improve model fit. The main effects of age are evaluated in a separate test because I wish to test the two terms (linear and quadratic) together.

```{r}
mod.3.gee =  geeglm(Status.n ~ Treatment + Visit.f + Center + Gender + Age1 + Age2 + Base + Treatment:Gender, id = ID, data = datalong, 
                   family ="binomial", corstr = "unstructured")
anova(mod.3.gee)[c(2,3,7),]

print("Hypothesis: age has no effect:")
wald.test(vcov(mod.3.gee),coef(mod.3.gee),c(9,8))
```

The main effect of visit does not significantly improve model fit (chisq(3) = 3.56, p = 0.3132). I conclude that the odds of good respiratory status are the same at every post-treatment visit and do not change over time. The main effect of center significantly improves model fit (chisq(1) = 8.43, p = 0.0037). I conclude that the odds of good respiratory status are different for subjects at different centers. Together, the linear and quadratic main effects of age significantly improve model fit (chisq(2) = 12.4, p = 0.0021). I conclude that there is a quadratic relationship between age and the odds of good respiratory status, and that individuals at different ages have different odds of good respiratory status. The main effect of baseline status significantly improves model fit (chisq(1) = 26.29, p < .0001). I conclude that the odds of good respiratory status post-treatment are different for subjects who had good and poor respiratory status at baseline, respectively.

Based on these results, I retain the main effects of age, center and baseline status and remove the main effect of visit. The preliminary final model is shown below:

```{r}
mod.4.gee =  geeglm(Status.n ~ Treatment + Center + Gender + Age1 + Age2 + Base + Treatment:Gender, id = ID, data = datalong, 
                   family ="binomial", corstr = "unstructured")
summary(mod.4.gee)
```

Before interpreting this model, I must evaluate the model fit. The assumptions of the generalized logistic regression model are relatively unrestrictive compared to some other types of regression models. There is no assumption of independence; on the contrary, I assume that the observations are correlated and explicitly account for this correlation. The choice of an unstructured covariance matrix means I make no assumption about the specific form of the dependence between observations in the same subject over time. There is no assumption that errors follow a particular distribution as in the linear model.

There are two fit-related issues that should be explored:

1) If the model is correctly specified, it should fit equally well at all levels of predicted odds. I can test this assumption with the Hosmer-Lemeshow test. 

```{r}
with(datalong,hoslem.test(Status.n,mod.4.gee$fitted.values))
```

The result of the Hosmer-Lemeshow test does not provide evidence for lack of fit.

2) Model fit should not depend to a great extent on any one observation or covariate pattern.

Although the model does not rely on the assumption of normally distributed errors, a normal QQ plot of residuals is a good way to identify observations which are poorly fit compared to all others.

```{r}
qqnorm(mod.4.gee$residuals)
```

There are two observations with extreme negative residuals. Complete data for the two individuals who produced these observations are shown below

```{r}
out.tabl = cbind(which(mod.4.gee$residuals< -10),mod.4.gee$residuals[which(mod.4.gee$residuals< -10)],datalong$ID[which(mod.4.gee$residuals< -10)])
colnames(out.tabl) = c("Observation #","Residual","Subject ID")
out.tabl
#datalong[datalong$ID %in% c(54,83),1:8]
data1[data1$ID %in% c(54,83),]
```

These observations come from visit 2 in subject 83 and visit 4 in subject 54. Both these individuals had good respiratory status at baseline and at every other visit, and they received the active treatment, so I would predict that they have high odds of good respiratory status at those visits. The fact that I have a few poorly fit observations is not a problem in and of itself, but it would be worthwhile to see what happens to the coefficient estimates when the model is refit without them. If the coefficient estimates change a great deal, this would suggest that these estimates reflect sampling error more than trends in the population from which I am sampling. In that case, one might be unable to replicate these results with an independent sample.

```{r}
print("% change in coefficients:")
mod.4.gee.trim = update(mod.4.gee,subset = -c(216,330))
change.coef(mod.4.gee, mod.4.gee.trim)
```

The proportional change in the coefficient estimates after removing these observations is relatively small. The one exception to this is the main effect of gender which, as I will show later, is not statistically significant anyway. There is no cause for alarm here.

Now I will begin to interpret the model. The table below gives point and range estimates for the fold difference in odds associated with various conditions.

```{r}
sum.4.gee = summary(mod.4.gee)
table.4.bounds = data.frame(matrix(nrow=4,ncol=3))
table.4.bounds[2:5,] = with(sum.4.gee$coefficients[c(2,3,4,7),],cbind(Estimate,Estimate+Std.err*qnorm(.025),Estimate+Std.err*qnorm(.975)))
SE.trtM = sqrt(vcov(mod.4.gee)[2,2]+vcov(mod.4.gee)[8,8]+2*vcov(mod.4.gee)[2,8])
table.4.bounds[1,] =  with(sum.4.gee$coefficients,sum(Estimate[c(2,8)])+c(0,SE.trtM*qnorm(.025),SE.trtM*qnorm(.975)))

table.4.dif = exp(table.4.bounds)
colnames(table.4.dif) = c("Estimate","95% Lower Bound","95% Upper Bound")
rownames(table.4.dif) = c("Active vs Placebo - Male","Active vs Placebo - Female","Center 2 (vs 1)","Male vs Female - Placebo","GRS at Baseline")
(table.4.dif)
```

The expected odds of GRS in female subjects receiving the active treatment is 18.15 times the expected odds in female subjects receiving placebo (95% CI from 3.74 to 88.04). One could also represent this as a percent difference and say that the odds are % 1715 higher in female subjects receiving the active treatment, but it can be hard to make sense of percentages that large. I use fold difference rather than percent difference for the other conditions to maintain consistency. The expected odds of GRS in male subjects receiving the active treatment is 2.82 times the expected odds in male subjects receiving placebo (95% CI from 1.278 to 6.23). The treatment appears to be effective for both male and female subjects, but more so for female subjects.

The expected odds of GRS at a post-treatment visit in subjects with GRS at baseline is 7.19 times that of subjects with PRS at baseline (95% CI from 3.529 to 14.66). The expected odds of GRS for subjects at center 2 are 1.77 times those at center 1 (95% CI from 0.838 to 3.74). The expected odds of GRS for male subjects receiving placebo is 1.11 times the expected odds in female subjects receiving placebo (95% CI from 0.367 to	3.36). The confidence interval for these last two include the null value of 1, meaning that I can not say for sure whether the subjects at center 2 actually have higher or lower odds than those at center 1 or whether males receiving placebo actually have higher or lower odds than females receiving placebo. 

The nonlinear relationship between age and odds of GRS is depicted in the plot below. The black line indicates the difference in estimated odds of GRS for individuals at a given age as a multiple of the estimated odds for an individual at the mean age in the sample (33.3 years old). The dotted line represents no difference. Naturally, the difference in the estimates at age 33.3 is none at all. The red lines represent 95% upper and lower confidence limits for the difference in the estimated odds. 

```{r}
age.x = min(datalong$Age):max(datalong$Age)-mean(data1$Age)
age2.x = age.x^2
y.lin = mod.4.gee$coefficients["Age1"]*age.x+mod.4.gee$coefficients["Age2"]*age2.x
SE.y.lin = sqrt(vcov(mod.4.gee)[5,5]*(age.x^2)+vcov(mod.4.gee)[6,6]*(age2.x^2)+vcov(mod.4.gee)[5,6]*age.x*age2.x*2)
low.y.lin = y.lin + SE.y.lin*qnorm(.025)
hi.y.lin = y.lin + SE.y.lin*qnorm(.975)

plot(NULL,xlim=c(10,70),ylim=c(0,9),ylab="Fold Difference",xlab = "Age",main = "Fold Difference in Odds of GRS (Vs Mean Age 33.3 yr)")
lines(min(datalong$Age):max(datalong$Age),(exp(y.lin)))
lines(min(datalong$Age):max(datalong$Age),(exp(low.y.lin)),col=2)
lines(min(datalong$Age):max(datalong$Age),(exp(hi.y.lin)),col=2)
abline(h=1,lty=2)
legend("top",fill=c(1,2),legend=c("Estimate","95% CI"))
```

From about ages 11 to 30, the odds of GRS are significantly higher than for subjects at the mean age of 33.3. Estimated odds of GRS reach their nadir around age 40. I'm skeptical of a model that says that eleven-year-olds have eight times the odds of GRS than 33-year-olds, so I will examine a plot of residuals by age to make sure that the model fits well at all ages.

```{r}
plot(datalong$Age,mod.4.gee$residuals,ylab="Residuals",xlab="Age")
```

The model seems to fit well across the range of ages in the sample and not systematically misclassify observations in one direction or another at any age.

In conclusion, the active treatment is effective compared to placebo, but has a greater effect in female than male patients. It appears to be equally effective at all ages, study centers, and for patients with good or poor respiratory status before treatment begins. Patients with GRS before treatment are more likely to have GRS in the future than those who start with PRS, all other things being equal. Patients at different ages have different odds of GRS, with patients around age 40 having the lowest odds of GRS and younger and older patients having greater odds of GRS. The odds of GRS do not appear to change over time for patients receiving either treatment regimen.

**References**

Stokes, M.E., Davis, C.S. and Koch, G.G. (1995). Categorical Data Analysis using the SAS System. Cary, NC: SAS Institute, Inc.  
  
Fitzmaurice, G. M., Laird, N. M., & Ware, J. H. (2011). Applied Longitudinal Analysis. Hoboken: Wiley.  
